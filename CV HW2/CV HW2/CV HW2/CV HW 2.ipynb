{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1174b83-2b42-44b8-a4da-c1e4341eb200",
   "metadata": {},
   "source": [
    "# HW 2 Computer Vision Course \n",
    "\n",
    "This assignment is designed to assess your understanding of key concepts in computer vision, image filtering techniques, and edge detection.\n",
    "\n",
    "**Submission Guidelines:**\n",
    "\n",
    "- Notebook Submission: Complete your solutions within this notebook file.\n",
    "\n",
    "- PDF Submission: Export your completed notebook as a PDF and submit it alongside the notebook.\n",
    "\n",
    "**Important Notes:**\n",
    "\n",
    "- Submissions without a PDF file will incur a **10% penalty on the total grade**.\n",
    "\n",
    "- Ensure that your code runs without errors and that all visual outputs are included in the PDF.\n",
    "\n",
    "- Write clear and concise explanations for your code and make sure to **add comments in each part of your code**.\n",
    "\n",
    "- Code with no comments will **lose 10%** of the question mark.\n",
    "\n",
    "- Properly format your code and comments to enhance readability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940be28d-c9c8-4aee-80d1-9b1141551d67",
   "metadata": {},
   "source": [
    "## MCQs Part 1 (10 marks)\n",
    "*note: to answer just type x within the brackets with no space around x*\n",
    "1. What is the primary purpose of blob detection in computer vision?\n",
    "- [ ] Detecting edges in an image\n",
    "- [ ] Identifying regions that differ in intensity from their surroundings\n",
    "- [ ] Finding corners in an image\n",
    "- [ ] Enhancing image resolution\n",
    "\n",
    "2. Why is the Harris corner detector not scale-invariant?\n",
    "- [ ] It is too sensitive to lighting changes.\n",
    "- [ ] It only works on grayscale images.\n",
    "- [ ] The same window cannot detect keypoints at different scales\n",
    "- [ ] It requires high-resolution images.\n",
    "\n",
    "3. To detect larger corners, what is required?\n",
    "- [ ] Smaller windows\n",
    "- [ ] The same size windows\n",
    "- [ ] No windows are needed.\n",
    "- [ ] Larger windows.\n",
    "\n",
    "4. What does the Laplacian of Gaussian (LoG) filter do?\n",
    "- [ ] Detects edges at a single scale\n",
    "- [ ] Detects blobs at different scales by changing the radius of the filter\n",
    "- [ ] Enhances image contrast\n",
    "- [ ] Reduces image noise\n",
    "\n",
    "5. What is the relationship between the Laplacian of Gaussian (LoG) filter and the Gaussian filter?\n",
    "- [ ] LoG is the inverse of the Gaussian filter.\n",
    "- [ ] LoG is the second derivative of the Gaussian filter\n",
    "- [ ] LoG is the same as the Gaussian filter.\n",
    "- [ ] LoG is the first derivative of the Gaussian filter.\n",
    "\n",
    "6. What is the Difference of Gaussian (DoG) used for?\n",
    "- [ ] Enhancing image contrast\n",
    "- [ ] reducing image noise\n",
    "- [ ] Efficiently approximating the LoG\n",
    "- [ ] Detecting edges\n",
    "\n",
    "7. Which of the following is a key component of local feature extraction?\n",
    "- [ ] Image compression\n",
    "- [ ] Description\n",
    "- [ ] Image enhancement\n",
    "- [ ] Image encryption\n",
    "\n",
    "8. What is the first step in the SIFT descriptor calculation?\n",
    "- [ ] Form weighted histogram\n",
    "- [ ] Concatenate 16 histograms\n",
    "- [ ] Find the orientation of the keypoint\n",
    "- [ ] Find the magnitude of the keypoint\n",
    "\n",
    "9. What does the acronym RANSAC stand for?\n",
    "- [ ] Random Sample Averaging and Classifying\n",
    "- [ ] RAndom Sample Consensus\n",
    "- [ ] Robust Analysis and Statistical Classification\n",
    "- [ ] Recursive Algorithm for Noise and Signal Analysis\n",
    "\n",
    "10. What is the initial step in the RANSAC algorithm?\n",
    "- [ ] Scoring model parameters\n",
    "- [ ] Solving for model parameters\n",
    "- [ ] Sampling the number of points required to fit the model\n",
    "- [ ] Finding the optimal solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23557327-9906-485f-b0d1-0f30d07deb42",
   "metadata": {},
   "source": [
    "## Theoritical questions Part 2 (6 marks)\n",
    "1. How does the Difference of Gaussian (DoG) approximate the Laplacian of Gaussian (LoG), and why is this approximation useful? (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4728c1-bb38-444b-98c2-fdb481d6de71",
   "metadata": {},
   "source": [
    "ANS:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b2c70-d780-4c4e-a1ea-6ad49d27453c",
   "metadata": {},
   "source": [
    "2. What is the purpose of the Nearest Neighbour Distance Ratio in feature matching, and how does it improve the accuracy of matches? (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02759b2c-04c0-4385-aa19-ed7b64d670e8",
   "metadata": {},
   "source": [
    "ANS:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec1ff3a-5e6d-43cc-8dd8-f5faa572472f",
   "metadata": {},
   "source": [
    "3.  Explain the RANSAC algorithm and its role in model fitting and alignment. How does RANSAC handle outliers in the data? (2 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b5729f-9b23-4c63-a446-79774a511177",
   "metadata": {},
   "source": [
    "ANS:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3650a2d-1efa-4378-8db0-df52344c9c61",
   "metadata": {},
   "source": [
    "## Coding questions Part 3 (22 marks)\n",
    "### SIFT Descriptor Matching for Object Detection (7 Marks)\n",
    "Using the SIFT descriptor, detect key points in two images and match the key points to identify similar regions between them. You may use OpenCV’s `BFMatcher` or `FLANN` for matching, and visualize the matching key points on the image.\n",
    "\n",
    "Steps to follow:\n",
    "\n",
    "- Extract SIFT descriptors from both images. (2 marks)\n",
    "- Use a matching algorithm to find corresponding points between the two images. (2 marks)\n",
    "- Display the images side by side with lines connecting matched key points. (2 marks)\n",
    "- Use the `img1.jpg` and `img2.jpg` images to test your function. (1 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81586796-6b6a-42ce-ac00-31931c7f014b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c0fa530-d9ab-4025-b79a-d1b92acc44c5",
   "metadata": {},
   "source": [
    "### Image Stitching using RANSAC and SIFT (15 marks)\n",
    "SIFT and RANSAC can be used to create panoranic scense that we usually have as an ooption in our phones. Now you need to create that by stitching 2 images. We provided 3 groups of images each are a pair that you need to stitch as one image.\n",
    "\n",
    "Steps to follow:\n",
    "- Write a **function** to read the images and convert them to gray. The function should return the gray image, original, and RGB image. (1 mark)\n",
    "- Extract SIFT descriptors from both images. (2 marks)\n",
    "- Create a **matching function** that uses BFMatcher to find corresponding points between the two images and then filter the matches according to a certain threshold that the user can specify. (4 marks)\n",
    "- Then you will need to use the provided function to show the matches on one image which is concatination of both images.(1 mark)\n",
    "- Use ransac to find the inliers and the homography (H) needed. Play around with the threshold and number of iterations to get reasonably good output. Plot the matches. (3 marks)\n",
    "- Then use the provided stitch function to get the stitched image (1 mark)\n",
    "- You should do this process on all 3 pairs of images **(A_L , A_R ,B_L , B_R ,C_L , C_R)** (3 marks) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "305994e9-b44f-4efc-8e46-29d27626c508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to gray function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c289baf5-45dc-4bf5-9c29-c971d1fc127c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bae9554-0769-4455-8d01-ce277fafd43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT descriptors on the gray images here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27626662-8485-421c-aa0a-549d3681e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ur matching function here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abd12305-5a46-4a31-a97b-19888d1fd381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot matching function\n",
    "def plot_matches(matches, total_img):\n",
    "    match_img = total_img.copy()\n",
    "    offset = total_img.shape[1]/2\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.imshow(np.array(match_img).astype('uint8')) #　RGB is integer type\n",
    "    \n",
    "    ax.plot(matches[:, 0], matches[:, 1], 'xr')\n",
    "    ax.plot(matches[:, 2] + offset, matches[:, 3], 'xr')\n",
    "     \n",
    "    ax.plot([matches[:, 0], matches[:, 2] + offset], [matches[:, 1], matches[:, 3]],\n",
    "            'r', linewidth=0.5)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0701ec7-9b4d-4853-b308-bec735ee0d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the plot_matches function here\n",
    "# total_img is the concatinated image of left and right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5fefeb-6904-4675-b320-d365b2ee917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def homography(pairs):\n",
    "    rows = []\n",
    "    for i in range(pairs.shape[0]):\n",
    "        p1 = np.append(pairs[i][0:2], 1)\n",
    "        p2 = np.append(pairs[i][2:4], 1)\n",
    "        row1 = [0, 0, 0, p1[0], p1[1], p1[2], -p2[1]*p1[0], -p2[1]*p1[1], -p2[1]*p1[2]]\n",
    "        row2 = [p1[0], p1[1], p1[2], 0, 0, 0, -p2[0]*p1[0], -p2[0]*p1[1], -p2[0]*p1[2]]\n",
    "        rows.append(row1)\n",
    "        rows.append(row2)\n",
    "    rows = np.array(rows)\n",
    "    U, s, V = np.linalg.svd(rows)\n",
    "    H = V[-1].reshape(3, 3)\n",
    "    H = H/H[2, 2] # standardize to let w*H[2,2] = 1\n",
    "    return H\n",
    "\n",
    "def random_point(matches, k=4):\n",
    "    idx = random.sample(range(len(matches)), k)\n",
    "    point = [matches[i] for i in idx ]\n",
    "    return np.array(point)\n",
    "\n",
    "def get_error(points, H):\n",
    "    num_points = len(points)\n",
    "    all_p1 = np.concatenate((points[:, 0:2], np.ones((num_points, 1))), axis=1)\n",
    "    all_p2 = points[:, 2:4]\n",
    "    estimate_p2 = np.zeros((num_points, 2))\n",
    "    for i in range(num_points):\n",
    "        temp = np.dot(H, all_p1[i])\n",
    "        estimate_p2[i] = (temp/temp[2])[0:2] # set index 2 to 1 and slice the index 0, 1\n",
    "    # Compute error\n",
    "    errors = np.linalg.norm(all_p2 - estimate_p2 , axis=1) ** 2\n",
    "\n",
    "    return errors\n",
    "\n",
    "def ransac(matches, threshold, iters):\n",
    "    num_best_inliers = 0\n",
    "    \n",
    "    for i in range(iters):\n",
    "        points = random_point(matches)\n",
    "        H = homography(points)\n",
    "        \n",
    "        #  avoid dividing by zero \n",
    "        if np.linalg.matrix_rank(H) < 3:\n",
    "            continue\n",
    "            \n",
    "        errors = get_error(matches, H)\n",
    "        idx = np.where(errors < threshold)[0]\n",
    "        inliers = matches[idx]\n",
    "\n",
    "        num_inliers = len(inliers)\n",
    "        if num_inliers > num_best_inliers:\n",
    "            best_inliers = inliers.copy()\n",
    "            num_best_inliers = num_inliers\n",
    "            best_H = H.copy()\n",
    "            \n",
    "    print(\"inliers/matches: {}/{}\".format(num_best_inliers, len(matches)))\n",
    "    return best_inliers, best_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9529eda2-9967-4acd-86ab-628a6b63b14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the ransac function with different parameters here while also plotting the inliners using the plot_matches function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb86e32-7cc0-4ac4-93c9-59a4b5763bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_img(left, right, H):\n",
    "    print(\"stiching image ...\")\n",
    "    \n",
    "    # Convert to double and normalize. Avoid noise.\n",
    "    left = cv2.normalize(left.astype('float'), None, \n",
    "                            0.0, 1.0, cv2.NORM_MINMAX)   \n",
    "    # Convert to double and normalize.\n",
    "    right = cv2.normalize(right.astype('float'), None, \n",
    "                            0.0, 1.0, cv2.NORM_MINMAX)   \n",
    "    \n",
    "    # left image\n",
    "    height_l, width_l, channel_l = left.shape\n",
    "    corners = [[0, 0, 1], [width_l, 0, 1], [width_l, height_l, 1], [0, height_l, 1]]\n",
    "    corners_new = [np.dot(H, corner) for corner in corners]\n",
    "    corners_new = np.array(corners_new).T \n",
    "    x_news = corners_new[0] / corners_new[2]\n",
    "    y_news = corners_new[1] / corners_new[2]\n",
    "    y_min = min(y_news)\n",
    "    x_min = min(x_news)\n",
    "\n",
    "    translation_mat = np.array([[1, 0, -x_min], [0, 1, -y_min], [0, 0, 1]])\n",
    "    H = np.dot(translation_mat, H)\n",
    "    \n",
    "    # Get height, width\n",
    "    height_new = int(round(abs(y_min) + height_l))\n",
    "    width_new = int(round(abs(x_min) + width_l))\n",
    "    size = (width_new, height_new)\n",
    "\n",
    "    # right image\n",
    "    warped_l = cv2.warpPerspective(src=left, M=H, dsize=size)\n",
    "\n",
    "    height_r, width_r, channel_r = right.shape\n",
    "    \n",
    "    height_new = int(round(abs(y_min) + height_r))\n",
    "    width_new = int(round(abs(x_min) + width_r))\n",
    "    size = (width_new, height_new)\n",
    "    \n",
    "\n",
    "    warped_r = cv2.warpPerspective(src=right, M=translation_mat, dsize=size)\n",
    "     \n",
    "    black = np.zeros(3)  # Black pixel.\n",
    "    \n",
    "    # Stitching procedure, store results in warped_l.\n",
    "    for i in tqdm(range(warped_r.shape[0])):\n",
    "        for j in range(warped_r.shape[1]):\n",
    "            pixel_l = warped_l[i, j, :]\n",
    "            pixel_r = warped_r[i, j, :]\n",
    "            \n",
    "            if not np.array_equal(pixel_l, black) and np.array_equal(pixel_r, black):\n",
    "                warped_l[i, j, :] = pixel_l\n",
    "            elif np.array_equal(pixel_l, black) and not np.array_equal(pixel_r, black):\n",
    "                warped_l[i, j, :] = pixel_r\n",
    "            elif not np.array_equal(pixel_l, black) and not np.array_equal(pixel_r, black):\n",
    "                warped_l[i, j, :] = (pixel_l + pixel_r) / 2\n",
    "            else:\n",
    "                pass\n",
    "                  \n",
    "    stitch_image = warped_l[:warped_r.shape[0], :warped_r.shape[1], :]\n",
    "    return stitch_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757ab533-d563-4bd8-8101-fd53a8a478c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the stitching function on the rgb version of your image\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
